# Сравнение: текущее решение vs кэш vs полноценные агенты (Assistants)

**Дата:** 26.02.2026  
**Контекст:** Pack pipeline (Concept → Boss → Captions ∥ Scenes → Critic), OpenAI.

Сравнение трёх вариантов доставки системного промпта модели:
1. **Текущее:** Chat Completions, system + user каждый раз в теле запроса.
2. **Кэш:** тот же Chat Completions + Prompt Caching (автоматический или с оптимизацией).
3. **Полноценные агенты:** Assistants API, инструкции хранятся в Assistant, в запросе только thread + сообщение.

Оценка по трём критериям: **скорость**, **стоимость**, **качество**.

---

## Сводная таблица

| Критерий   | Текущее (prompt каждый раз) | Кэш (Prompt Caching)        | Полноценные агенты (Assistants)   |
|-----------|-----------------------------|-----------------------------|-----------------------------------|
| **Скорость** | Базовая латентность          | До ~80% быстрее при cache hit | Сопоставимо или хуже (runs async, polling) |
| **Стоимость** | Полная цена за input/output  | До ~90% дешевле input при cache hit | Та же цена за токены; доп. сложность |
| **Качество** | Одинаково                    | Одинаково                    | Одинаково                          |

---

## 1. Скорость (латентность)

### Текущее решение
- Каждый вызов: отправка полного `messages` (system + user), модель обрабатывает все input-токены и генерирует ответ.
- Латентность = сеть + очередь + **prefill (все input)** + **decode (output)**.
- Prefill тем дольше, чем больше input-токенов. У нас system prompt ~500–1500 токенов на агента, user — от десятков до сотен.

### Кэш (Prompt Caching)
- При **cache hit** префикс (наш system prompt) уже обработан на сервере — prefill идёт только для **новой части** (user message) + decode.
- Документация OpenAI: *«Prompt Caching can reduce latency by up to 80%»* при cache hit.
- Условия: prompt ≥1024 токенов; статичный контент в начале (у нас уже так: system, потом user). Кэш срабатывает автоматически, без смены API.
- Ограничение: один и тот же префикс должен попадать на одну и ту же машину; при >~15 req/min на один префикс часть запросов уходит на другие машины (cache miss). Для одного пайплайна мы делаем по одному вызову на агента — в пределах одного пака кэш почти не помогает. Выигрыш появляется при **нескольких паках подряд** (или параллельно): второй и последующие паки могут получить cache hit по Concept/Boss/Captions/Scenes/Critic.
- Можно явно задать `prompt_cache_key` (например, по имени агента), чтобы стабилизировать роутинг и повысить hit rate при повторных запусках.

**Итог по скорости:** кэш даёт ускорение только при повторных вызовах одного и того же агента в окне 5–10 мин (или 24h при extended). В одном пайплайне — без изменений. При нагрузке с несколькими паками — до заметного ускорения.

### Assistants API
- Инструкции хранятся в объекте Assistant; в запросе передаётся `thread_id` + новое сообщение. Объём данных в запросе меньше.
- Но: Run ассистента — асинхронная операция (create run → polling или streaming). Реальная латентность «до первого токена» и «до конца» может быть такой же или хуже из‑за накладных расходов и очередей Runs. Нет гарантии, что «не гонять промпт» даёт быстрее ответ.
- Плюс рефакторинг: вместо одного синхронного вызова — создание/переиспользование треда, добавление сообщения, запуск run, ожидание завершения, чтение ответа из сообщений треда.

**Итог по скорости:** переход на Assistants ради скорости не обоснован; кэш даёт выигрыш при повторяющихся вызовах в рамках текущего API.

---

## 2. Стоимость

### Текущее решение
- Оплата за все input-токены (system + user) и output-токены по тарифу модели. Каждый вызов — полная стоимость.

### Кэш (Prompt Caching)
- **Cached input tokens** тарифицируются со скидкой **50%** (по текущей политике OpenAI). До 90% снижения стоимости input возможно при высоком cache hit rate (большая доля токенов в префиксе).
- Кэш включается автоматически для промптов ≥1024 токенов (gpt-4o и новее). Доплаты за использование кэша нет.
- В одном пайплайне каждый агент вызывается один раз — кэш при первом запуске не помогает. При втором и последующих паках (в окне удержания кэша) — экономия на input по Concept, Boss, Captions, Scenes, Critic.

**Итог по стоимости:** кэш снижает стоимость input при повторных вызовах; для одного пака за сессию эффект нулевой, при активной нагрузке — заметная экономия.

### Assistants API
- Токены (input/output) тарифицируются так же, как в Chat Completions. Инструменты (Code Interpreter, File Search и т.д.) мы не используем — доп. платы за них нет.
- То есть при той же длине контекста и том же выводе **стоимость по токенам та же**. Экономии «за счёт того, что промпт не гоняем» нет — на бэкенде модель всё равно получает полный контекст (инструкции + тред).

**Итог по стоимости:** Assistants не дают выигрыша по деньгам для нашего сценария; кэш даёт экономию при повторных вызовах.

---

## 3. Качество

- Во всех трёх вариантах модель в итоге получает одни и те же инструкции (system) и то же пользовательское задание (user/thread message).
- Документация OpenAI: *«Prompt Caching does not influence the generation of output tokens or the final response»*.
- **Вывод:** качество ответов не зависит от выбора варианта (текущий / кэш / Assistants). Отличия только в скорости и стоимости.

---

## 4. Практические выводы

| Цель                         | Рекомендация |
|-----------------------------|--------------|
| Ускорить один пайплайн      | Кэш и Assistants не дают выигрыша; важнее лимиты `max_tokens`, быстрые модели, объём промпта (уже делали). |
| Снизить стоимость при росте нагрузки | Включить использование кэша: убедиться, что system идёт первым, длина ≥1024 токенов; при необходимости задать `prompt_cache_key` по агенту. Assistants не экономят. |
| «Не гонять промпт» с клиента | Только Assistants: промпт хранится на стороне OpenAI, в запросе только thread + сообщение. Цена и скорость при этом не улучшаются. |
| Минимум изменений в коде    | Оставить текущий Chat Completions; кэш уже учитывается автоматически при подходящих промптах. При желании добавить `prompt_cache_key` и (для части моделей) `prompt_cache_retention: "24h"` для стабильного hit rate. |

---

## 5. Что можно сделать в коде без перехода на Assistants

1. **Проверить длину system prompt в токенах** для каждого агента. Если <1024 — кэш не применяется; можно чуть расширить инструкции или не разбивать на части без нужды.
2. **Добавить `prompt_cache_key`** в вызовы Chat Completions (если API это поддерживает для используемой модели): например, `concept`, `boss`, `captions`, `scenes`, `critic`. Это улучшает роутинг и повышает вероятность cache hit при нескольких паках.
3. **Опционально:** для моделей с extended caching задать `prompt_cache_retention: "24h"`, чтобы кэш жил дольше и чаще срабатывал при редких запусках пака.

После этого текущее решение остаётся тем же по архитектуре, но лучше использует кэш по скорости и стоимости при повторных вызовах. Переход на Assistants оправдан только если нужна именно модель «полноценных агентов» (треды, история, инструменты), а не экономия или ускорение.
